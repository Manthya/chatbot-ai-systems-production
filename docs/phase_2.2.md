# Phase 2.2: Observability & Context Scalability

## 1. Overview

**Goal:** Transform the Chatbot AI System from a simple persistent chat into a **scalable, observable production system**. This phase addresses two critical issues:
1.  **Blindness:** Lack of visibility into token usage, latency, and model versions.
2.  **Instability:** Context window overflows causing crashes in long conversations.

**Status:** âœ… Completed

## 2. Key Implementations

### A. Observability Layer
We enhanced the database schema to capture "meta-metrics" for every message. This allows us to track costs and performance without needing external tools immediately.

**Schema Changes (`Message` Table):**
*   `token_count_prompt` (Int): Exact tokens consumed by the user context.
*   `token_count_completion` (Int): Exact tokens generated by the model.
*   `latency_ms` (Int): End-to-end processing time for the response.
*   `model` (Str): The specific model version used (e.g., `qwen2.5:14b-instruct`), crucial for debugging regressions.

**Data Flow:**
1.  `OllamaProvider` extracts usage statistics from the underlying API (via the final stream chunk).
2.  `ChatOrchestrator` captures these metrics during the streaming process.
3.  `ConversationRepository` persists them transactionally alongside the message content.

### B. Sliding Window Context (The "Hot" Memory)
To prevent the LLM context from exceeding its limit (e.g., 8k, 32k tokens) and to maintain low latency, we implemented a sliding window mechanism.

**Mechanism:**
*   **Legacy Behavior:** Fetch *all* messages (`SELECT *`). Latency grows linearly with conversation length.
*   **New Behavior:** Fetch *last N* messages (`SELECT * ... ORDER BY seq DESC LIMIT 50`).
*   **Result:** Latency is capped (O(1)), and the system never crashes due to context overflow, regardless of conversation length.

**Code:**
```python
# repositories/conversation.py
async def get_recent_messages(self, conversation_id: UUID, limit: int = 50) -> List[Message]:
    # Efficiently fetch the tail of the conversation
    statement = (
        select(Message)
        .where(Message.conversation_id == conversation_id)
        .order_by(Message.sequence_number.desc())
        .limit(limit)
    )
    # ...
```

## 3. Validation

We verified the implementation with `scripts/verify_observability.py`.

**Test Results:**
*   **Sliding Window:** Confirmed that for a 60-message conversation, only the last 50 were loaded.
*   **Persistence:** Confirmed that `token_count` and `latency` were correctly saved and retrieved.

## 4. Next Steps

With the "Hot" memory (Sliding Window) secured, we lose access to older messages in the immediate context. The next logical step is **Layer 2 Memory (Summarization)** to retain the "gist" of the conversation as messages slide out of the window.
