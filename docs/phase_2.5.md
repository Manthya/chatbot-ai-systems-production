# Phase 2.5: Advanced Memory & Observability

## 1. Overview

**Goal:** Transform the Chatbot AI System into a production-grade application by implementing a **Hybrid Memory Architecture** and a robust **Observability Layer**. This ensures the bot can handle long conversations efficiently, recall information from the distant past, and provide developers with operational insights.

**Status:** âœ… Completed

---

## 2. Key Implementations

### A. Observability Layer
We promoted critical metrics from generic metadata to first-class database columns in the `Message` table.

*   **Metrics Tracked:**
    *   `token_count_prompt`: Tokens consumed by user query + context.
    *   `token_count_completion`: Tokens generated by the model.
    *   `latency_ms`: Time taken for the LLM to respond.
    *   `model`: The specific model version used (e.g., `qwen2.5:14b-instruct`).
*   **Benefit:** Enables cost tracking, performance monitoring, and regression testing.

### B. 3-Layer Hybrid Memory ("Memory Layer Cake")
We implemented three distinct layers of memory to balance fidelity, context limits, and long-term recall.

#### 1. Hot Memory: Sliding Window (Phase 2.6)
*   **Mechanism:** Only the last 50 messages are provided to the LLM in the immediate message history.
*   **Why:** Prevents "Context Overflow" crashes and keeps inference latency constant as the conversation grows.

#### 2. Warm Memory: Running Summarization (Phase 2.7)
*   **Mechanism:** Every 20 messages, the system generates a summary of the conversation history. This summary is injected into the system prompt.
*   **Why:** Retains the "gist" and sequence of events for messages that have slid out of the "Hot" memory window.

#### 3. Cold Memory: Vector Search / RAG (Phase 3)
*   **Mechanism:** Every message is embedded into a 768-dimension vector using `nomic-embed-text` and stored using `pgvector`.
*   **Why:** Allows the bot to perform a semantic search across ALL past conversations to find specific details (e.g., names, dates, facts) that are no longer in the window or summary.

---

## 3. How a User Query Uses the 3-Layer Architecture

When a user sends a message, the system intelligently combines all three layers to provide a response that is both fast and contextually deep.

### Step-by-Step Flow:
1.  **Retrieve Hot Memory (Window)**:
    *   The system immediately pulls the **last 50 messages** from the database. This ensures the bot remembers what you just said a few seconds ago with 100% accuracy.
2.  **Retrieve Warm Memory (Summary)**:
    *   The system looks for the most recent **Conversation Summary**. Even if the conversation has 1,000 messages, the summary tells the bot the "story so far" in just a few sentences.
3.  **Retrieve Cold Memory (Vector Search)**:
    *   The system converts the user's question into a "vector" and searches millions of past messages for a match. If you ask about a specific detail from last week, this is how the bot finds it.
4.  **The LLM Brain**:
    *   The bot receives: **[Summary] + [Retrieved Facts] + [Last 50 Messages]**. It uses this combined context to write a perfect answer.
5.  **Background Updates**:
    *   Once the answer is sent, the system updates the "Cold Memory" with the new message and checks if it's time to refresh the "Warm Memory" summary.

---

## 4. Infrastructure Updates
*   **Vector Database:** Migrated to `pgvector/pgvector:pg16` Docker image.
*   **Embedding Model:** Integrated `nomic-embed-text` via Ollama for efficient local embeddings.
*   **Migrations:** Successfully applied schema changes for `summary`, `last_summarized_seq_id`, and `embedding` columns.

---

## 4. Validation & Experiments
We verified the entire stack via automated scripts and real-world experiments:

1.  **Observability Test:** `scripts/verify_observability.py` confirmed metric persistence.
2.  **Summarization Test:** `scripts/verify_summarization.py` confirmed threshold logic and summary updates.
3.  **Vector Search Test:** `scripts/verify_vector_search.py` confirmed semantic retrieval.
4.  **Integrated Experiment:** Successfully recalled a "forgotten" fact after it was pushed out of a 10-message sliding window using a combination of all three layers.

---

## 5. Conclusion
The Chatbot AI System is now stable, observable, and possesses "infinite" memory. It is ready for deployment in complex, long-running conversational scenarios.
